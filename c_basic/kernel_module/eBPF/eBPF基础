1.eBPF应用程序, 从开发到运行的典型流程如下:
	* 利用Clang, 将C语言开发的代码编译为eBPF object文件

	* 在用户空间将eBPF object文件载入内核; 
		载入前, 可能对object文件进行各种修改; 
		这一步骤, 可能通过iproute2之类的BPF ELF loader完成, 也可能通过自定义的控制程序完成;

	* BPF Verifier在VM中进行安全性校验;

	* JIT编译器将字节码编译为机器码, 返回BPF程序的文件描述符;

	* 使用文件描述符将BPF程序挂钩到某个子系统(例如networking)的挂钩点; 
		子系统有可能将BPF程序offload给硬件(例如智能网卡)

	* 用户空间通过eBPF Map和内核空间交换数据, 获知eBPF程序的执行结果





2.挂钩点
	eBPF程序以事件驱动的方式执行, 具体来说, 就是在内核的代码路径上, 存在大量挂钩点(Hook Point); 
	eBPF程序会注册到某些挂钩点, 当内核运行到挂钩点后, 就执行eBPF程序; 

	挂钩点主要包括以下几类: 
		* 网络事件, 例如封包到达
		* Kprobes / Uprobes
		* 系统调用
		* 函数的入口/退出点

	在以上的4类挂载点中, 你都可以插入eBPF程序, 实现eBPF程序的功能;
	(也非常容易做网络设备监控, 检测, 包过滤等操作)





3.BPF Verifier(eBPF程序执行前的安检)
	在加载之后, BPF校验器负责验证eBPF程序是否安全, 它会模拟所有的执行路径, 并且: 
		* 检查程序控制流, 发现循环
		* 检测越界的跳转、不可达指令
		* 跟踪Context的访问、栈移除
		* 检查unpriviledged的指针泄漏
		* 检查助手函数调用参数



4.BPF JITs:
	在校验之后, eBPF程序被JIT编译器编译为Native代码; 



5.BPF Maps
	'kv键值对'的形式存储, 通过文件描述符来定位, 值是不透明的Blob(任意数据); 
	用于跨越多次调用共享数据, 或者与用户空间应用程序共享数据; 
	一个eBPF程序可以直接访问最多64个Map, 多个eBPF程序可以共享同一Map; 
	(eBPF程序内部的数据共享机制, 类似redis 一样的东西, 可以在eBPF程序之间共享数据, 也可以与用户空间的程序共享数据)



6.Pinning
	BPF Maps和程序都是内核资源, 仅能通过文件描述符访问到; 
	文件描述符对应了内核中的匿名inodes; 

	用户空间程序可以使用大部分基于文件描述符的API, 但是文件描述符是限制在进程的生命周期内的, 这导致Map难以被共享; 
	比较显著的例子是iproute2, 当tc或XDP加载eBPF程序之后, 自身会立刻退出; 
	这导致无法从用户空间访问Map; 

	为了解决上面的问题, 引入了一个最小化的、内核空间中的BPF文件系统; 
	BPF程序和Map会被pin到一个被称为object pinning的进程; 
	bpf系统调用有两个命令BPF_OBJ_PIN、BPF_OBJ_GET分别用于钉住、取回对象; 

	tc这样的工具就是利用Pinning在ingress/egress端共享Map; 



7.尾调用
	尾调用允许一个BPF程序调用另外一个, 这种调用没有函数调用那样的开销; 
	其实现方式是long jump, 重用当前stack frame; 
	注意: 
		只用相同类型的BPF程序才能相互尾调用; 

	要使用尾调用, 需要一个BPF_MAP_TYPE_PROG_ARRAY类型的Map, 其内容目前必须由用户空间产生, 值是需要被尾调用的BPF程序的文件描述符; 
	通过助手函数bpf_tail_call触发尾调用, 内核会将此调用内联到一个特殊的BPF指令; 



8.BPF-BPF调用
	BPF-BPF调用是一个新添加的特性; 
	在此特性引入之前, 典型的BPF C程序需要将所有可重用的代码声明为always_inline的, 这样才能确保LLVM生成的object包含所有函数; 
	这会导致函数在每个object文件中都反复(只要它被调用超过一次)出现, 增加体积; 



